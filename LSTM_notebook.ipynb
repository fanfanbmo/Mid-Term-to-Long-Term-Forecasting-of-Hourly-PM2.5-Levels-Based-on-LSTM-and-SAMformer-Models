{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('AQI.csv')\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "data.set_index('time')\n",
    "time = data['time']\n",
    "\n",
    "fold = 3\n",
    "# Split into training, validation, and test sets according to time\n",
    "# fold 1\n",
    "if fold == 1:\n",
    "    train = data[(time >= '2018-01-01 00:00:00') & (time <= '2018-12-31 23:00:00')]\n",
    "    validation = data[(time >= '2019-01-01 00:00:00') & (time <= '2019-12-31 23:00:00')]\n",
    "# fold 2\n",
    "if fold == 2:\n",
    "    train = data[(time >= '2018-01-01 00:00:00') & (time <= '2019-12-31 23:00:00')]\n",
    "    validation = data[(time >= '2020-01-01 00:00:00') & (time <= '2020-12-31 23:00:00')]\n",
    "# fold 3\n",
    "if fold == 3:\n",
    "    train = data[(time >= '2018-01-01 00:00:00') & (time <= '2020-12-31 23:00:00')]\n",
    "    validation = data[(time >= '2021-01-01 00:00:00') & (time <= '2021-12-31 23:00:00')]\n",
    "\n",
    "\n",
    "test = data[(time >= '2022-01-01 00:00:00') & (time <= '2022-12-31 23:00:00')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Validation shape: {validation.shape}')\n",
    "print(f'Test shape: {test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "feature_scaler = StandardScaler()\n",
    "train_features_scaled = feature_scaler.fit_transform(train.drop(columns=['time']))\n",
    "validation_features_scaled = feature_scaler.transform(validation.drop(columns=['time']))\n",
    "test_features_scaled = feature_scaler.transform(test.drop(columns=['time']))\n",
    "\n",
    "# Normalize the target\n",
    "target_scaler = StandardScaler()\n",
    "train_target_scaled = target_scaler.fit_transform(train[['PM2.5']])\n",
    "validation_target_scaled = target_scaler.transform(validation[['PM2.5']])\n",
    "test_target_scaled = target_scaler.transform(test[['PM2.5']])\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "def create_sequences(features, target, seq_length, pred_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(features) - seq_length - pred_length + 1):\n",
    "        x = features[i:i + seq_length]\n",
    "        y = target[i + seq_length:i + seq_length + pred_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 120\n",
    "pred_length = 12\n",
    "\n",
    "X_train, y_train = create_sequences(train_features_scaled, train_target_scaled, seq_length, pred_length)\n",
    "X_val, y_val = create_sequences(validation_features_scaled, validation_target_scaled, seq_length, pred_length)\n",
    "X_test, y_test = create_sequences(test_features_scaled, test_target_scaled, seq_length, pred_length)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "print(f'X_val shape: {X_val.shape}, y_val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Input(shape=(seq_length, X_train.shape[2])),\n",
    "    LSTM(300, activation='softsign', return_sequences=True),\n",
    "    LSTM(300, activation='softsign', return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(pred_length, kernel_regularizer=tf.keras.regularizers.l2(0.03))\n",
    "])\n",
    "\n",
    "# Compile the model with MAE as an additional metric\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=1e-10)\n",
    "\n",
    "# save running time\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Training the Model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Making Predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "running_time = end - start\n",
    "print(f'Running time: {running_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving predictions to npy files\n",
    "np.save('y_pred_train_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_pred_train)\n",
    "np.save('y_pred_val_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_pred_val)\n",
    "np.save('y_pred_test_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_pred_test)\n",
    "np.save('y_train_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_train)\n",
    "np.save('y_val_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_val)\n",
    "np.save('y_test_'+str(seq_length)+'_'+str(pred_length)+'.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from npy files\n",
    "y_pred_train = np.load('y_pred_train_'+str(seq_length)+'_'+str(pred_length)+'.npy')\n",
    "y_pred_val = np.load('y_pred_val_'+str(seq_length)+'_'+str(pred_length)+'.npy')\n",
    "y_pred_test = np.load('y_pred_test_'+str(seq_length)+'_'+str(pred_length)+'.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape y_train, y_val, y_test\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1]))\n",
    "y_val = y_val.reshape((y_val.shape[0], y_val.shape[1]))\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1]))\n",
    "\n",
    "# print shape\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse scaling\n",
    "y_train = target_scaler.inverse_transform(y_train)\n",
    "y_val = target_scaler.inverse_transform(y_val)\n",
    "y_test = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "y_pred_train = target_scaler.inverse_transform(y_pred_train)\n",
    "y_pred_val = target_scaler.inverse_transform(y_pred_val)\n",
    "y_pred_test = target_scaler.inverse_transform(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the time index back to the predictions\n",
    "y_pred_train = pd.DataFrame(y_pred_train, index=train['time'].iloc[seq_length:seq_length + len(y_pred_train)])\n",
    "y_pred_val = pd.DataFrame(y_pred_val, index=validation['time'].iloc[seq_length:seq_length + len(y_pred_val)])\n",
    "y_pred_test = pd.DataFrame(y_pred_test, index=test['time'].iloc[seq_length:seq_length + len(y_pred_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the time index back to the target\n",
    "y_train = pd.DataFrame(y_train, index=train['time'].iloc[seq_length:seq_length + len(y_train)])\n",
    "y_val = pd.DataFrame(y_val, index=validation['time'].iloc[seq_length:seq_length + len(y_val)])\n",
    "y_test = pd.DataFrame(y_test, index=test['time'].iloc[seq_length:seq_length + len(y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model with RMSE and MAE\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "\n",
    "# when calculating rmse and mae for test set, we need to exclude missing values\n",
    "# load missing_indexes.csv\n",
    "missing_indexes = pd.read_csv('C:/Users/zhang/Desktop/Thesis/Data/missing_indexes.csv')\n",
    "# only use the test set\n",
    "missing_indexes = missing_indexes.loc[35064:]\n",
    "# delete the index column\n",
    "missing_indexes = missing_indexes.drop('time', axis=1)\n",
    "# reshape missing_indexes\n",
    "def create_sequences(df, pred_length, seq_length):\n",
    "    ys = []\n",
    "    for i in range(len(df) - seq_length - pred_length + 1):\n",
    "        y = df[i + seq_length:i + seq_length + pred_length]\n",
    "        ys.append(y)\n",
    "    return np.array(ys)\n",
    "missing_indexes_sequence = create_sequences(missing_indexes, pred_length, seq_length)\n",
    "# reshape missing_indexes_sequence\n",
    "missing_indexes_sequence = missing_indexes_sequence.reshape(missing_indexes_sequence.shape[0], missing_indexes_sequence.shape[1])\n",
    "# mask missing values\n",
    "test_rmse = np.sqrt(np.mean((y_test[~missing_indexes_sequence] - y_pred_test[~missing_indexes_sequence])**2))\n",
    "test_mae = np.mean(np.abs(y_test[~missing_indexes_sequence] - y_pred_test[~missing_indexes_sequence]))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.2f}, Train MAE: {train_mae:.2f}')\n",
    "print(f'Validation RMSE: {val_rmse:.2f}, Validation MAE: {val_mae:.2f}')\n",
    "print(f'Test RMSE: {test_rmse:.2f}, Test MAE: {test_mae:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to a csv file\n",
    "import csv\n",
    "with open('results.csv', mode='a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # write header if file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['seq_length', 'pred_length', 'train_rmse', 'train_mae', 'val_rmse', 'val_mae', 'test_rmse', 'test_mae', 'running_time'])\n",
    "    writer.writerow([seq_length, pred_length, train_rmse, train_mae, val_rmse, val_mae, test_rmse, test_mae, running_time])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average of 96 hours so that there is only one value for each day\n",
    "y_pred_train_avg = pd.DataFrame()\n",
    "y_pred_train_avg['PM2.5'] = y_pred_train.mean(axis=1)\n",
    "y_pred_val_avg = pd.DataFrame()\n",
    "y_pred_val_avg['PM2.5'] = y_pred_val.mean(axis=1)\n",
    "y_pred_test_avg = pd.DataFrame()\n",
    "y_pred_test_avg['PM2.5'] = y_pred_test.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average\n",
    "y_train_avg = pd.DataFrame()\n",
    "y_train_avg['PM2.5'] = y_train.mean(axis=1)\n",
    "y_val_avg = pd.DataFrame()\n",
    "y_val_avg['PM2.5'] = y_val.mean(axis=1)\n",
    "y_test_avg = pd.DataFrame()\n",
    "y_test_avg['PM2.5'] = y_test.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot trend graph (train)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_train_avg, label='True')\n",
    "plt.plot(y_pred_train_avg, 'r', label='Predicted')\n",
    "plt.title('AQI training Prediction')\n",
    "legend = plt.legend(loc='upper left', shadow=True, fontsize='x-large')\n",
    "\n",
    "# save the plot\n",
    "plt.savefig('train_trend_' + str(seq_length) + '_' + str(pred_length) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize average results with dot plot (validation)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_val_avg, label='True')\n",
    "plt.plot(y_pred_val_avg, 'r', label='Predicted')\n",
    "plt.title('AQI validation Prediction')\n",
    "plt.legend()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig('val_trend_' + str(seq_length) + '_' + str(pred_length) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visyalize average results with dot plot (test)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test_avg, label='True')\n",
    "plt.plot(y_pred_test_avg, 'r', label='Predicted')\n",
    "plt.title('AQI test Prediction')\n",
    "plt.legend()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig('test_trend_' + str(seq_length) + '_' + str(pred_length) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss curve and save\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "# save the plot\n",
    "plt.savefig('loss_curve_' + str(seq_length) + '_' + str(pred_length) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
